{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-13T16:03:00.667360Z",
     "start_time": "2025-03-13T16:02:54.131380Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase\n",
    "from transformers.utils import PaddingStrategy"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pingzhili/miniconda3/envs/hf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b969daa453a90f32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:23:13.255363Z",
     "start_time": "2025-03-13T16:23:13.209751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/Users/pingzhili/huggingface-repo/allenai/OLMoE-1B-7B-0125-Instruct\",\n",
    "                                          trust_remote_code=True)\n",
    "examples = {\n",
    "    \"question\": [\"Is 123 a prime?\"],\n",
    "    \"response\": [\"No, 123 is not a prime number. It can be factored as 3 Ã— 41.\"]\n",
    "}"
   ],
   "id": "25ab71c2beb0c6a6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T10:27:46.100209Z",
     "start_time": "2025-03-04T10:27:46.092086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_general_chat_template(\n",
    "        question: str,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        response: Optional[str] = None,\n",
    "):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    if response is None:\n",
    "        return tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    else:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "\n",
    "\n",
    "def sft_olmoe_train_batch_preprocess_fn(\n",
    "        examples: Dict[str, List[Any]],\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "):\n",
    "    if tokenizer is None:\n",
    "        raise ValueError(\"Tokenizer is required for SFT training.\")\n",
    "\n",
    "    # 1. apply general chat template to each example\n",
    "    all_chat_texts = []\n",
    "\n",
    "    for question, response in zip(examples[\"question\"], examples[\"response\"]):\n",
    "        chat_text = apply_general_chat_template(question, response=response, tokenizer=tokenizer)\n",
    "        all_chat_texts.append(chat_text)\n",
    "\n",
    "    # 2. Tokenize the chat\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_labels = []\n",
    "\n",
    "    for chat_text in all_chat_texts:\n",
    "        encoded = tokenizer(chat_text, padding=False, truncation=True)\n",
    "        input_ids = encoded[\"input_ids\"]\n",
    "        attention_mask = encoded[\"attention_mask\"]\n",
    "\n",
    "        # 3. Only apply LM loss on the assistant's response & \"<|endoftext|>\"\n",
    "        labels = [-100] * len(input_ids)\n",
    "\n",
    "        assistant_token_id = tokenizer(\"<|assistant|>\", add_special_tokens=False)[\"input_ids\"]\n",
    "        end_token_id = tokenizer.convert_tokens_to_ids(\"|||IP_ADDRESS|||\")\n",
    "\n",
    "        pos_assistant = -1\n",
    "        pos_end_after_response = -1\n",
    "\n",
    "        i = 0\n",
    "        while i <= len(input_ids) - len(assistant_token_id):\n",
    "            matched = True\n",
    "            for j in range(len(assistant_token_id)):\n",
    "                if input_ids[i + j] != assistant_token_id[j]:\n",
    "                    matched = False\n",
    "                    break\n",
    "\n",
    "            if matched:\n",
    "                pos_assistant = i + len(assistant_token_id) - 1\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        if pos_assistant != -1:\n",
    "            for i in range(pos_assistant + 1, len(input_ids)):\n",
    "                if input_ids[i] == end_token_id:\n",
    "                    pos_end_after_response = i\n",
    "                    break\n",
    "\n",
    "        if pos_assistant != -1 and pos_end_after_response != -1:\n",
    "            for i in range(pos_assistant + 1, pos_end_after_response):\n",
    "                labels[i] = input_ids[i]\n",
    "\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "        all_labels.append(labels)\n",
    "        print(pos_assistant, pos_end_after_response)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_masks,\n",
    "        \"labels\": all_labels\n",
    "    }"
   ],
   "id": "850f6b144359d86e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T10:27:47.611276Z",
     "start_time": "2025-03-04T10:27:47.603597Z"
    }
   },
   "cell_type": "code",
   "source": "results = sft_olmoe_train_batch_preprocess_fn(examples, tokenizer)",
   "id": "ad08806d4a8a1193",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 49\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T10:46:21.173360Z",
     "start_time": "2025-03-04T10:46:21.163661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs):\n",
    "    \"\"\"\n",
    "    Pads without triggering the warning about how using the pad function is sub-optimal when using a fast tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    # To avoid errors when using Feature extractors\n",
    "    if not hasattr(tokenizer, \"deprecation_warnings\"):\n",
    "        return tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "\n",
    "    # Save the state of the warning, then disable it\n",
    "    warning_state = tokenizer.deprecation_warnings.get(\"Asking-to-pad-a-fast-tokenizer\", False)\n",
    "    tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "    try:\n",
    "        padded = tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "    finally:\n",
    "        # Restore the state of the warning.\n",
    "        tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = warning_state\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomDataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.0 (Volta).\n",
    "        return_tensors (`str`, *optional*, defaults to `\"pt\"`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "    extra_keys_to_ignore: Optional[List[str]] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        if \"label\" in features:\n",
    "            features[\"labels\"] = features[\"label\"]\n",
    "            del features[\"label\"]\n",
    "        if \"label_ids\" in features:\n",
    "            features[\"labels\"] = features[\"label_ids\"]\n",
    "            del features[\"label_ids\"]\n",
    "\n",
    "        features_to_ignore = {\n",
    "            k: [item[k] for item in features] for k in self.extra_keys_to_ignore\n",
    "        } if self.extra_keys_to_ignore else {}\n",
    "        features = [\n",
    "            {k: v for k, v in feature.items() if k not in self.extra_keys_to_ignore} for feature in features\n",
    "        ] if self.extra_keys_to_ignore else features\n",
    "\n",
    "        # take labels out of features\n",
    "        labels_batch = [{\"input_ids\": feature[\"labels\"]} for feature in features]  # Fake name for padding\n",
    "        features = [{k: v for k, v in feature.items() if k != \"labels\"} for feature in features]\n",
    "        batch = pad_without_fast_tokenizer_warning(\n",
    "            self.tokenizer,\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        labels_batch = pad_without_fast_tokenizer_warning(\n",
    "            self.tokenizer,\n",
    "            labels_batch,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        if self.tokenizer.pad_token_id is not None:\n",
    "            labels_batch[\"input_ids\"][labels_batch[\"input_ids\"] == self.tokenizer.pad_token_id] = -100\n",
    "        labels_batch[\"labels\"] = labels_batch[\"input_ids\"]\n",
    "        del labels_batch[\"input_ids\"]\n",
    "        batch = {**batch, **features_to_ignore, **labels_batch}\n",
    "        return batch\n"
   ],
   "id": "5757188921efbe29",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T10:46:25.588509Z",
     "start_time": "2025-03-04T10:46:25.586628Z"
    }
   },
   "cell_type": "code",
   "source": "data_collator = CustomDataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8, max_length=1024)",
   "id": "bab383aae58b85c5",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T10:47:32.033140Z",
     "start_time": "2025-03-04T10:47:32.030543Z"
    }
   },
   "cell_type": "code",
   "source": "batch = [{\"input_ids\": [1, 2, 3, 4, 5, 6], \"labels\": [-100, -100, 3, 4, 5, 6]}]",
   "id": "e665d09b5f813d38",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T10:47:35.382539Z",
     "start_time": "2025-03-04T10:47:35.362612Z"
    }
   },
   "cell_type": "code",
   "source": "data_collator(batch)",
   "id": "21047ec46d5ade6b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pingzhili/miniconda3/envs/hf/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,     2,     3,     4,     5,     6, 50280, 50280]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0]]),\n",
       " 'labels': tensor([[-100, -100,    3,    4,    5,    6, -100, -100]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "45e55c15c59c797f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
