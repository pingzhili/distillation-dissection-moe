# **Distillation Dissection in MoE:**

## **Introduction**

Mixture-of-Experts (MoE) models use a **gating network** (router) to assign each input token’s representation to one or more specialized “experts” (small feed-forward networks) . Over training, the gating learns to select only the experts that make the best predictions for a given input, causing each expert to become **specialized** on particular inputs or tasks . In practice, MoE gating typically picks the top-$k$ experts for each token (e.g. top-2 in GShard, top-1 in Switch Transformer, or top-$k$ in general) and combines their outputs . An auxiliary load-balancing loss is often used to ensure no expert is overloaded, nudging the router to distribute tokens more evenly across experts . However, overly uniform routing can hurt specialization – if every expert is equally consulted on all data, experts may just duplicate the same core knowledge . Recent MoE architectures like DeepSeekMoE address this by introducing **shared experts** (experts always active for every token) to capture common knowledge, allowing the remaining **routed experts** to focus on more unique, non-overlapping knowledge . In summary, MoE models balance between **expert specialization** (each expert handles a distinct subset of inputs) and **balanced expert utilization** (avoiding idle or overloaded experts).

In our setup, we have a *teacher* MoE model A (the 16B-parameter **MoonLight-MoE**) and two 1.3B-parameter *student* MoE models (B and C) based on **OLMoE** architecture. Model A uses 64 experts with **top-6 routing \+ 2 shared experts** (so effectively 8 experts active per token), across 27 layers (hidden size 2048). Model B and C each use 64 experts with **top-8 routing** (no always-on experts) in a 16-layer Transformer (hidden size 2048). Model B is a baseline fine-tuned on the **Tülu** multi-task instruction dataset, whereas model C is a distilled student: it was trained to imitate model A’s outputs on the same dataset (knowledge distillation). The Tülu dataset contains diverse instruction-following tasks (math problems, coding, Q\&A, etc.) . Our goal is to design a fine-grained experimental framework to **distinguish whether model C was distilled from A**, as opposed to just fine-tuned like B, by analyzing MoE-specific internal behaviors. We assume no strict compute limitations, enabling extensive probing of these models. We will leverage a held-out **calibration dataset** (with similar distribution to Tülu) to validate that any observed differences generalize ining set.

## **Paths**

### 1\. Model A, B, and C checkpoints:

- Model A (teacher): \`moonshotai/Moonlight-16B-A3B-Instruct\`  
- Model B (SFTed model): checkpoints/olmoe-1b-7b-0125-sft-original-filtered/checkpoint-10000  
- Model C (Distilled model from A): \`checkpoints/olmoe-1b-7b-0125-sft-distilled-moonlight-filtered/checkpoint-10000\`

Reference code for usage: \`scripts/dump-hidden-states.py\`

### 2\. Dumped hidden states and router outputs:

- Model A (teacher): \`outputs/profiling/moonlight-base\`  
- Model B (SFTed model): \`outputs/profiling/olmoe-sft-original-10000\`  
- Model C (Distilled model from A): \`outputs/profiling/olmoe-sft-distill-10000\`

#### Data Structure: \`router\_tokens.pt\` (under each model’s dumped outputs directory)

1\. \*\*Module-specific data\*\*: For each MoE module in the model (identified by its name), a nested dictionary containing:

   \- \`"input"\`: List of tensors representing the input hidden states to each MoE module before routing decisions

     	\- Each tensor has shape \`\[sequence\_length, hidden\_dim\]\` (batch dimension is squeezed)
    
     	\- Collected across multiple batches of input data

   \- \`"selected\_experts"\`: List of tensors containing the indices of experts selected by the router

     	\- For OLMoE: Shape corresponds to token positions and selected experts (top-k indices)
    
     	\- For Moonlight: Shape matches the token positions with the selected expert indices
    
     	\- Collected across the same batches as the inputs

2\. \*\*Input token IDs\*\*: A list of token ID tensors under the key \`"input\_ids"\`

   \- Each tensor represents the token IDs for a batch from the dataset

   \- Allows mapping routing decisions back to specific tokens

3\. \*\*Input sample sources\*\*: A list of strings under the key \`"source"\`

   \- Each string represents the input sample's source (for example, "ami_aime"), there are 28 sources with 10 samples per source.

   \- Allows mapping routing decisions back to specific source (i.e. task)

This data structure provides a comprehensive view of expert routing decisions across different MoE modules, enabling analysis of which experts handle which tokens throughout the model's layers when processing various inputs.

### **Profiling data**:

We use `Phando/sft-dataset-valid` for profiling, with 280 samples in total, 10 samples per source and 28 sources in total. Each sample contains "question", "response", and "source" (all strings). Reference usage is in \`scripts/dump-hidden-states.py\`.


## **Key MoE Properties for Analysis**

To identify subtle effects of distillation on an MoE, we first define several fine-grained MoE-specific properties and behaviors to compare between models:

* **Expert Specialization:** The degree to which different experts in the MoE have carved out distinct roles or input domains. High specialization means each expert handles a focused subset of inputs (e.g. specific task types), whereas low specialization means experts’ duties overlap. Specialization can be quantified by how uneven the expert usage distribution is (more skewed \= more specialized) . We expect model A to have strong specialization (by design, with shared experts capturing common skills and others specialized ). We will testpreserves similar expert specializations from A (due to distillation) more so than B.

* **Routing Distribution & Load Balance:** This refers to how the router distributes tokens among experts across the dataset – is usage balanced or do a few experts dominate? We measure **expert usage frequency** for each model and the overall entropy of the usage distribution . A perfectly uniform allocation of tokens to experts yields high entropy, whereas if each token type goes mostly to one expert, the distribution entropy is low (indicating clustered routing) . We also examine per-task routing: which experts are most used for math vs code vs QA tasks, etc. Balanced routing is encouraged by auxiliary losses in training , but too much balance can cause redundant experts . We will see if distillation caused C’s routing profile to more closely mimic A’s (which had a particular balance due to 2 always-on experts) compared to B’s routing.

* **Router Confidence and Smoothness:** We define this as how **confident or peaked the gating decisions** are for each token. A confident router strongly prefers a particular set of experts (low uncertainty), whereas a “softer” routing distributes weight more evenly or changes experts more gradually. One metric is the **per-token gating entropy** – lower entropy means the router picks experts with higher certainty (one or two dominate the probability mass), while higher entropy means the router is less confident (weights spread across experts). Another view is the **router’s margin** (difference between the top gate score and others). We will analyze the distribution of gating entropy for each model, and how often the router’s top choice is much larger than the second choice. We consider **routing smoothness** as well: if we slightly perturb an input or look at successive tokens in a sequence, does the expert selection remain stable or flip unpredictably? (Prior work adds noise to gating to promote smoother transitions in routing .) Smoother routing might manifest as more gradual changes in expert assignments across similar inputs and potentially a higher average entropy (less brittle hard decisions). We hypothesize that the distilled model C may exhibit **smoother, more calibrated gating** behavior than B. Distillation via soft targets can make a student’s predictions less over-confident ; analogously, C’s router might not be as extreme in its gating decisions, perhaps yielding more distributed (smooth) expert usage, whereas B might lean into whichever experts it found useful with less subtlety.

* **Hidden Activation Similarity:** If model C has truly learned to “think like” model A internally (not just matching outputs), its hidden layer activations and intermediate representations might align better with A  Knowledge distillation aims to transfer not only outputs but also some of the teacher’s internal reasoning patterns . We will compare the **hidden states** of the models on the same inputs. This can be done at various levels: the final logits or embeddings, the outputs of each MoE layer, or even the channel-wise (neuron) activations. We will use measures like **Centered Kernel Alignment (CKA)** to quantify representation similarity between models . Additionally, we can look at **channel-level activations** correlation – for each hidden dimension or each expert’s output, compare activation patterns between models. A distilled student should show higher similarity to the teacher in these metrics than a non-distilled model would.

* **Expert Collaboration Patterns:** With top-$k$ routing, multiple experts can be active for a single token, effectively collaborating on that token’s representation. We examine how experts co-activate and interact. For example, do certain experts frequently get selected together as a team for some inputs? Are there *groups* of experts that handle particular domains in tandem? We can analyze the **co-occurrence frequency** of expert pairs in the selected top-$k$ sets and construct a co-activation graph or heatmap. We’ll also examine if model C mirrors A’s patterns of expert co-utilization (e.g. if A’s shared experts are always active alongside a task-specific expert, does C develop analogous “generalist” experts that frequently accompany others?). Expert collaboration also relates to how many experts effectively contribute significant activation per token – we might measure the **effective number of experts** (perhaps via the Gini or entropy of the per-token allocation weights among the top-$k$). Distillation might lead C to use combinations of experts similar to the teacher’s strategy (for instance, always using a general knowledge expert plus a specialized one), whereas B may have different or less consistent combinations.

* **Router Parameter Alignment:** Beyond behavior, we can directly inspect the learned **router parameters** (the gating network weights) to see if model C’s gating function is quantitatively closer to A’s than B’s is. The gating network in these Transformers is typically a linear projection from the hidden state to a 64-dimensional logit (one per expert) . We can compare these weight matrices. Because expert indices between A and the students might not correspond one-to-one (expert \#5 in A could play a similar role as expert \#17 in C, for example), we will consider **permutation-aligned similarity**: for each expert in the student, find the most similar expert’s weight vector in the teacher (via cosine similarity), and see how the distribution of these similarities looks for model C vs B. A higher number of strong matches or a higher overall correlation would indicate C’s router learned a similar gating policy to A. This is a fine-grained check to dllation caused any convergence of gating logic (even implicitly). Note that model A has additional gating parameters for the 2 shared experts (which are always selected by design ), whereas B and C do not have dedicated shared experts – we will account for this by focusing comparisons on the routed expert weights.

Using these properties, we next design a series of experiments to **quantitatively compare model B and model C with respect to model A**. Each experiment targets one or more of the above aspects, specifying metrics and visualizations to reveal whether model C shows telltale signs of having been distilled from A. Throughout, our plan includes analyzing both the Tülu task data and a held-out calibration set to ensure robustness.

## **Research Questions**

Based on the above, we formalize key research questions to guide our experiments:

1. **Expert Specialization:** Does the distilled student (C) exhibit expert specialization patterns (task assignments, usage skew) closer to the teacher (A) than the fine-tuned model (B) does? For instance, are the same experts in C heavily responsible for the same task types as in A?

2. **Routing Behavior:** How do the token-level routing decisions of C compare to A and B? Is C’s router output distribution (e.g. entropy, confidence) more similar to A’s, indicating *smoother* or more refined roesult of distillation?

3. **Representation Alignment:** To what extent has distillation aligned C’s internal representations with A’s? Does C’s hidden state space (or output logits) more closely match the teacher’s, relative to B’s alignment with the teacher?

4. **Expert-Expert Interaction:** Are there subtle effects of distillation on how experts are utilized in combination? For example, does C reuse certain experts (analogous to A’s shared experts) across all inputs, or show similar expert co-activation groupings as A?

5. **Parameter Similarity:** Can we detect any direct similarities in the learned parameters (router weights or expert feed-forward weights) between A and C that are absent in B, supporting that C inherited some of A’s structure?

With these questions in mind, we outline experiments below that will address each one. Each experiment will detail the **experimental setup**, the **metrics and visualizations** we will use, and the **expected outcomes** if model C indeed carries hallmarks of being distilled from model A.

## **Experiment 1: Expert Usage and Specialization Analysis**

**Research Question:** Are the experts in model C specialized in similar ways to those in model A (unlike B)? Does C allocate tasks to experts similarly to its teacher?

**Setup & Procedure:** We will analyze the **expert utilization statistics** of each model on the Tülu dataset (and repeat on the calibration set). Key steps:

* **Task-wise Expert Usage:** Partition the evaluation data by task type (e.g. math vs. coding vs. QA, etc., as labeled in Tülu). For each model, compute how often each expert is activated for each task subset. This yields a matrix of usage frequencies (tasks × 64 experts). We will normalize these to get probability distributions per task of which experts are used.

* **Ove istribution:** Compute each expert’s overall fraction of tokens processed. From this, calculate the **entropy of the distribution of token assignments across the 64 experts** for each model. A lower entropy (more peaked distribution) indicates a few experts take most of the load (high specialization), whereas a higher entropy indicates more balanced usage (lower specialization). We will also compute the entropy per task to see if some tasks are handled by a small subset of experts (which would imply specialization by domain).

* **Expert Specialization Metrics:** For a more direct measure of specialization, we can compute for each expert an entropy of its task distribution (does each expert focus on one task or many?). We expect specialized experts to have low entropy over task categories (meaning an expert mainly handles one type of task). We will compare the distribution of these per-expert entropies for A, B, C. Another metric is **KL divergence** or **Jensen-Shannon divergence** between the task distribution of each expert in A vs the closest matching expert in C or B (to see if C has experts that function like A’s experts).

* **Visualization:** We will create a **heatmap of expert usage by task** for each model (experts on one axis, task categories on the other, color \= % of that task’s tokens routed to that expert). This allows visual comparison of specialization patterns. For example, in model A we might see distinct columns (experts) brightly associated with certain tasks (indicating that expert handles mostly that task). We will see if model C’s heatmap looks qualitatively closer to A’s than B’s does. We’ll also plot the **overall expert usage distribution** as bar charts for each model, perhaps side by side, and overlay the distribution for different datasets (Tülu vs calibration) to confirm consistency. Additionally, we’ll report the numerical **dispatch entropy** for each model (and per-task entropies) in a table.

**Expected Outcomes:** We anticipate model A will show pronounced specialization – e.g., certain experts dominating math problems, others dominating coding tasks, etc., and a relatively skewed overall usage (some experts used far more than others) consistent with prior MoE studies where experts pick up distinct “clusters” of inputs . Model B (fine-tuned without distillation) may show less clear specialization: since B was only supervised on task outputs, it might not have separated the tasks as cleanly among experts, potentially resulting in more uniform expert usage or some experts under-utilized (especially if fine-tuning was short or did not strongly exercise all experts). Model C, on the other hand, having been distilled from A’s behavior, is expected to mirror A’s specialization to a degree. For example, if in A expert \#42 is heavily used for coding tasks, we might find that in C some expert (not necessarily the same index 42, but one of them) is also predominantly used for coding tasks. The heatmap for C should show clusters of high usage that correspond to the same task columns as A’s heatmap, whereas B’s might be more diffuse. We expect C’s overall expert usage entropy to be closer to A’s (possibly lower than B’s, indicating more skew/specialization). If model A’s design included **2 shared experts** always active (handling common knowledge ), we will check if model C developed an analogous behavior: for instance, perhaps one or two experts in C end up being used across almost all tasks (high overall frequency), effectively acting as “generalists”. If so, that would be a strong indicator that C learned to mimic the teacher’s strategy of relying on shared experts for universal knowledge. Model B, lacking that guidance, might not have any experts with nearly universal usage. (If anything, B might either use many experts equally or inadvertently collapse to using a subset, but not in a purposeful shared-vs-specialist division). This experiment will give the first evidence of whether C’s expert allocation is **aligned with A’s specialization scheme** or just arbitrarily developed during fine-tuning.

## **Experiment 2: Per-Token Routing Patterns and Router Confidence**

**Research Question:** Does the distilled model C route individual inputs more similarly to the teacher? Are its gating decisions (which experts to pick and with what confidence) closer to A’s than B’s are? We aim to quantify how **aligned each token’s expert assignment** is between models, and measure differences in gating confidence and smoothness.

**Setup & Procedure:** We drill down to the **token level routing decisions** and gating outputs:

* **Token-wise Expert Overlap:** For a sample of input sequences (from both Tülu and calibration sets), we will capture the set of experts selected for each token by each model. We can then directly compare model C vs A and B vs A on a per-token basis. One metric is the **Jaccard similarity** between the set of experts chosen by A and the set chosen by the student (C or B) for the same token. We will compute the average Jaccard similarity across all tokens (or all sequences) for A-vs-C and A-vs-B. Another metric is simply the **overlap count** (how many of the top-$k$ experts chosen by A are also among the top-$k$ chosen by the student). For example, model A chooses experts {5,12,42, …} for token $t$; model C chooses {7,12,45,…}; the overlap count is 1 (expert 12 common). We will see if C has a systematically higher overlap with A than B does. This analysis might require aligning expert identities if they are permuted; however, since we are comparing set overlap by expert index, any alignment issue would hurt the measured overlap – finding higher overlap for C would be convincing evidence of shared routing behavior. To further guard against expert index permutation issues, we might use a content-based alignment: e.g., match experts by the type of tokens they handle (from Experiment 1\) before checking token-wise overlaps.

* **Gating Probability Distribution Comparison:** Instead of just which experts were chosen (which is after the top-$k$ selection), we can compare the raw gating output vectors. For each token, the router produces a score or probability for each of 64 experts. We will collect these 64-dimensional vectors from A, B, C for the same token. Then we can measure similarity metrics like **cosine similarity** or **KL divergence** between the distributions. We will evaluate the average similarity of C’s gating vector to A’s versus B’s to A’s. A high similarity for C would mean that for each token, C’s notion of which experts are suitable (and in what relative proportion) mirrors the teacher’s. This is a very fine-grained check – if distillation succeeded in transferring the routing policy, C’s gating outputs might align with A’s even if they ultimately pick perhaps slightly different top experts.

* **Router Confidence & Entropy:** For each token’s gating output, compute the **entropy** of the probability distribution over experts (considering only the top-$k$ normalized probabilities if needed, since outside top-$k$ may be zeroed in sparse routing). We will look at the distribution of these entropies for each model. We expect model A, which has highly specialized experts, to often have *low entropy* gating (i.e., one or a few experts clearly have high probability for a token, others negligible – the router is confident about who should handle the token). In fact, prior analyses show that as MoE training progresses, the router’s dispatch entropy tends to decrease, indicating it has discovered clear clusters for each expert . Model B might also show low entropy on tasks it is confident about, but if it hasn’t achieved the same level of specialization, it could sometimes have higher entropy (uncertain which expert to use, especially if multiple experts learned overlapping capabilities due to lack of a clear teacher signal). Model C could behave in two possible ways: (a) If distillation taught it the teacher’s confident routing, it might also have low entropy, confidently routing tokens in a manner similar to A. (b) Alternatively, if distillation encouraged using more of its capacity to mimic A’s complex behavior, C’s router might distribute weight a bit more (since A had 2 shared experts always contributing, C might compensate by often utilizing \~2 general experts for broad knowledge). We will verify this by comparing the **entropy histograms**: e.g., perhaps A and C both have a mode at very low entropy (very confident tokens) and a long tail of higher entropy for ambiguous tokens, whereas B’s distribution might differ.

* **Router Confidence Metrics:** We will also calculate the average **max gating probability** per token (i.e., the probability assigned to the top expert). A higher average max probability means more confidence. And we will examine the **margin** between the highest and second-highest gating scores per token; a large margin implies the router was clearly favoring one expert (high confidence in choice), whereas a small margin implies indecision. We expect A (top-6 \+ 2 shared) may have moderate margins because even if one expert is best, the shared experts will always take some share, possibly reducing the top-vs-second gap. B’s top-8 might sometimes concentrate heavily on one expert if it hasn’t found utility in using many, leading to large margins for some tokens. C might have learned to utilize multiple experts more like A, potentially resulting in smaller margins (since it might often mimic the teacher’s pattern of combining experts). We will quantify these trends.

* **Temporal Smoothness:** Using sequential data from the tasks, we will investigae the routing is across **consecutive tokens** in a sequence or slight input perturbations. For each model, pick a few example sequences and track which expert (or experts) get the highest gating weight for each token position. We can visualize this as a **routing trace plot**: x-axis token position, y-axis expert ID (or color-coded by expert), with linehen the chosen expert changes. If a model’s routing is “jittery,” we’ll see frequent oscillation in which expert is dominant from token to token. If it’s smoother, the expert in charge might persist for a span of tokens (perhaps corresponding to a semantic segment of the input). We suspect the teacher model A, being larger and well-trained, might have a stable routing pattern (since adding noise during training can smooth out router decisions ). Model C may inherit some of that stability, whereas B might exhibit more erratic routing if it hasn’t learned as refined a gating policy. We can also do a small experiment: take an input and make a minor edit (e.g., rephrase a question slightly) and see if the expert selection of each model changes drastically or remains similar – essentially testing the robustness of routing.

* **Visualization:** We will produce side-by-side **histograms** of gating entropy for A, B, C. We’ll also plot the cumulative distribution function (CDF) of the entropy to compare them quantitatively (e.g., what fraction of tokens have entropy below 1.0 for each model, etc.). We’ll include scatter plots of teacher vs student gating probabilities: for a random sample of tokens, plot teacher’s probability for a particular expert on the x-axis and student’s on the y-axis, to see if points cluster around the diagonal (high correlation) for C more than for B. A specialized visualization is a **heatmap of token-level expert overlaps**: index tokens along one axis (or clusters of similar tokens) and experts on another, highlighting matches between A and C vs A and B. However, since there are many tokens, a summary might be clearer: e.g., a bar chart showing the average Jaccard similarity of expert sets (A–C vs A–B), with error bars or distribution. Finally, the **routing trace plots** described will illustrate qualitative smoothness differences. We might embed a small example snippet (like a… token sequence along with the expert index having highest probability). These traces will visually demonstrate if model C’s routing transitions are more stable (like A’s) than B’s.

**Expected Outcomes:** We expect that for **token-level expert overlaps**, model C will have a higher overlap with A’s selections than model B does. For example, perhaps on average 5 of the top-8 experts chosen by A and C overlap, whereas A vs B might overlap only 3 out of 8\. Similarly, the Jaccard similarity of expert sets might be, say, 0.6 for A–C vs 0.4 for A–B. If C was distilled well, it likely learned to route inputs similarly to how A would. In terms of **gating probabilities**, C should also show higher distributional similarity to A. We might see a strong positive correlation (near-diagonal scatter) for A vs C gating weights, and weaker for A vs B. If model C indeed learned the teacher’s “routing policy,” the gating entropy and confidence metrics could reveal interesting patterns: possibly **lower entropy and higher confidence** similar to A’s. Alternatively, if the teacher’s use of shared experts (always on) made A’s gating outputs inherently different in form (since some probability mass is always on shared experts), we might find that C compensates by often giving some probability to a set of general experts – effectively imitating the teacher’s behavior by spreading probabilities in a consistent way. This could manifest as **bimodal** behavior in C’s entropy: maybe a cluster of tokens where it has very low entropy (like A does when one expert clearly wins) and another cluster where it maintains a couple of strong experts (mimicking teacher’s shared experts usage). B, by contrast, might sometimes be overconfident on a single expert (if fine-tuning caused it to rely heavily on a small subset of experts, it might have many tokens with extremely low entropy, but also other tokens where it’s confused due to lack of a guiding signal). We also expect C’s routing to be **smoother over sequences** – if A had stable routing, C likely learned to divide sequences similarly. B might exhibit abrupt changes in expert assignment if it didn’t internalize such a pattern. Overall, evidence from this experiment would strongly indicate whether C’s *token-wise behavior* aligns with A’s beyond just matching outputs.

## **Experiment 3: Hidden State and Representation Similarity**

**Research Question:** Beyond the routing decisions, doal representations (hidden activations) of model C align more with the teacher A than those of model B? This checks if distillation imparted some of the teacher’s *“way of thinking”*, not just the final answers【28†L139-L145】. We will also inspect expert output similarity specifically, to see if the actual transformed representations by each expert in C resemble those in A (expert alignment).

**Setup & Procedure:** We will use a combination of **Centered Kernel Alignment (CKA)** and direct hidden state comparisons:

* **Layer-wise CKA Similarity:** We measure the similarity between the hidden representations of two models given the same input. For each Transformer layer $\\ell$ (particularly MoE layers), we ut hidden vectors for all tokens in a sample of the dataset. We then compute CKA between model A’s layer $\\ell$ representations and model C’s la representations (after possibly projecting to the same dimensionality if needed; CKA can handle different dims【20†L23-L31】). Do this for each layer of C vs the corresponding (or conceptually closest) layer of A. We’ll do the same for model B vs A. Plotting **CKA similarity by layer** for A–B and A–C can show where the student matches the teacher. Likely, the later layers (closer to output) have higher similarity due to distillation focusing on outputs, but if C effectively learned intermediate behaviors, we might see high CKA in middle layers as well. B (just fine-tuned) might only be similar at the output layer and diverge more in lower layers. If model A has 27 layers and students have 16, we have some choices: possibly compare A’s last 16 MoE layers to the 16 layers of B/C, or use interpolation (e.g., compare every \~1.7th layer of A to one layer of B/C). We will choose a mapping such as comparing layer 27 of A to layer 16 of B/C (final outputs), and proportionally earlier ones. We can also compute an **overall CKA** on all layers concatenated or averaged.

* **Expert-Specific Hidden Similarity:** MoE layers output a mixture of expert computations. We can try to disentangle: take one MoE layer in model A and identify which expert processed each token (top-1 for simplicity). Do the same in model C for the analogous layer. Now compare the hidden outputs *from corresponding experts*. If, for a given token, A used expert i and C (on that same token) used expert j, it might be interesting if i and j correspond in some way. However, that’s complicated by differing expert identities. Instead, perhaps look at **expert output clusters**: cluster the hidden outputs of experts in A (which might naturally form groups by expert function), and see if C’s outputs form similar clusters. Another approach: For each expert in A, take the average output vector it produces (or PCA of its outputs) for some specific stimulus set; do the same for each expert in C, then compare those averages (cosine similarity). This may reveal if, say, expert 5 in A (which might be a “math expert”) has an analogous expert in C whose output vectors are close. In effect, we could attempt to “match” experts between A and C by comparing their output space centroids. If distillation transferred expertise, we’d expect each expert in A to find a counterpart in C with similar outputs for similar inputs. Model B, on the other hand, trained independently, might have experts that do not align with A’s in this way. We’ll quantify by finding the best matching expert in C for each expert of A (by similarity of their mean activation on a probe set) and measure the similarity score. Then do the same for B. We anticipate higher scores or more one-to-one correspondences for A–C.

* **Logit and Embedding Similarity:** At the final output level, since distillation directly optimizes student logits vs teacher logits (if done traditionally via KL divergence on soft targets【29†L13-L17】), we expect model C’s output logits or next-token probabilities to be much closer to A’s than B’s are (B was only trained on the hard labels/instructions). We can verify this by measuring the **KL divergence** between A’s and C’s output distributions (averaged over tokens), vs A’s and B’s. Also compute **top-$k$ overlap** in predictions between A and each student. This is more of a sanity check, because we know C was distilled to match outputs, but it’s worth confirming that indeed C’s outputs are significantly more aligned to A than B’s (especially on the calibration set which neither fine-tuning nor distillation directly optimized for). A notably lower KL divergence for A–C would confirm successful output distillation. However, the heart of this experiment is about internal states, so we won’t dwell on output too much beyond confirming that baseline.

* **Visualization:** We will present a **layer-wise CKA plot**: x-axis could be the “depth” (layer index normalized between 0 and 1 for both models since they have different depths), y-axis CKA similarity. Two lines: one for A–C, one for A–B. We expect the A–C line to be generally above the A–B line, especially in certain layers. If distillation was thorough, perhaps many layers of C track A, not just the last. If we find specific layers where A–C spikes in similarity (maybe corresponding to MoE layers), that would be insightful to point out. For expert-specific similarities, a **matrix of expert correspondence** could be visualized: e.g., a heatmap where rows are A’s experts, columns are C’s experts, and values are cosine similarity between their mean output embeddings on the same inputs. If C effectively learned the same experts, this matrix might show a strong diagonal structure (after appropriate permutation of experts) indicating a one-to-one alignment. For B, we might see a more noisy matrix with no clear one-to-one pattern. We might also use a **dendrogram or clustering plot** to show how experts from A, B, C intermingle or cluster based on their output features; ideally, each cluster would contain an expert from A and a corresponding expert from C together (if C mirrored it), whereas B’s might form separate clusters. And for logit similarity, perhaps a simple bar chart of average KL divergence (A–B vs A–C) or a small table of that and top-$k$ overlaps. A scatter plot of A’s vs student’s logits per token could also visualize that C’s points lie closer to the diagonal (perfect agreement) than B’s.

**Expected Outcomes:** We expect a significantly higher **representation similarity** between A and C than between A and B. For instance, overall CKA might be 0.8 for A–C vs 0.5 for A–B (hypothetically). Particularly, certain layers in C might show striking alignment (maybe those corresponding to key MoE layers where knowledge was distilled). If knowledge distillation via hidden state mimicry was used (some distillation techniques explicitly match hidden states, not just outputs【28†L139-L145】), then CKA could be high at multiple layers. Even if only logits were matched, a student often *implicitly* aligns some hidden features with the teacher to produce those logits, so we should still see higher CKA than the fine-tuned model which only saw task loss. Model B may have diverged internally (finding its own smaller-network solution to tasks). For the **expert matching**, we hope to find that for each major “expert role” in A, C has one serving a similar role, evidenced by high output similarity. For example, suppose A has an expert that always activates on math problems and transforms those inputs in a certain way; we might find an expert in C that, when it activates on the same math problems, produces embeddings that are close to those of A’s expert – meaning C learned to emulate that expert’s function. B’s experts, lacking that guidance, might not align with A’s; perhaps B solved math problems using a combination of different experts or an entirely different strategy, so no single expert in B produces outputs similar to A’s math expert. Seeing a diagonal in the expert-similarity heatmap for A–C (even if not perfect) would be strong evidence of expert-level knowledge transfer. The final output comparison likely shows C’s predictions (distribution over vocabulary) are much closer to A’s – indeed that’s expected by the design of distillation【29†L13-L22】. We should note if there’s any *over-distillation*: sometimes a distilled model can stick too closely to the teacher and not be as robust. If we observe on the calibration set that C’s outputs track A’s even when A might be wrong or uncertain, that could be a side-effect. B, trained on correct answers, might occasionally diverge (possibly doing better on some edge cases). But overall, this experiment’s expected result is confirming that **C internalized A’s representational geometry**, setting it apart from B.

## **Experiment 4: Expert Co-Activation and Collaboration Patterns**

**Research Question:** Did distillation impart any of the teacher’s patterns of *expert collaboration* and usage of shared experts? Specifically, does model C demonstrate similar **co-activation statistics** (which experts tend to be selected together) and **shared expert reuse** as model A, more so than model B does?

**Setup & Procedure:** We focus on analyzing how multiple experts are used simultaneously for tokens and if certain experts consistently appear as part of the top-$k$ selection (especially analogs of shared experts):

* **Co-Activation Frequency:** For each model, we take all instances of top-$k$ expert selections for tokens and compute a **co-activation matrix** $M$, where $M\_{ij}$ counts how many times expert $i$ and expert $j$ were both selected for the same token (in model outputs, normalized by total tokens). We can convert this to a correlation or lift over independent probability – basically measure if two experts are *positively correlated* in routing. This matrix will be symmetric and possibly highlight clusters of experts that often fire together. We will generate $M^A$, $M^B$, $M^C$ for the models.

* **Shared Expert Identification:** In model A, 2 experts are “shared” (always active) by design【18†L61-L69】. This means in every top-6 selection, those shared experts are present. So in A’s co-activation matrix, those shared experts (say indices S1 and S2) will have high co-activation with *every* other expert (because any time any expert j is chosen, S1 and S2 are there too). Thus, row S1 and S2 of $M^A$ would be almost uniformly high. We will see if model C has any experts that play a similar role. If C effectively learned that “for every input, I should always engage certain experts to provide general knowledge,” then there will be one or two experts in C that have an unusually broad co-activation: they appear alongside many different other experts frequently. We can detect this by looking for rows in $M^C$ that have high values across many columns (except maybe itself). Another way: calculate each expert’s **average co-activation rate** with any other expert (essentially the sum of a row in $M$ minus self-cooccurrences). Sort experts by this value. In model A, the top 2 by far will be the shared experts. In model C, if the top 1-2 experts also show a significantly higher co-activation rate than the rest, that indicates C developed “pseudo-shared” experts that are nearly always used. In model B, we suspect no such pattern unless by coincidence – more likely, co-activations will be dictated by task (e.g., maybe expert 5 and 12 often co-activate because together they solve coding tasks, but not across all tasks). We’ll quantitatively compare: e.g., what fraction of tokens does each expert participate in? This is similar to usage frequency (from Experiment 1\) but here we want to see if any expert in C has near 100% participation (like a shared expert would). If yes, strong evidence of distillation; if not, then perhaps C didn’t emulate that aspect.

* **Expert Clusters in Collaboration:** Using the co-activation matrix, perform a **clustering (like spectral clustering)** to identify groups of experts that tend to act together. For model A, we expect one cluster might be the set containing the shared experts plus possibly others (since shared experts connect to all). Other clusters might correspond to groups of experts that solve related tasks (they might not co-activate in the same token because usually each token picks distinct ones, but across sequences they might form complementary sets). However, because top-6 or top-8 means multiple experts per token, we might indeed see small groups that often come as a package. For example, maybe for a math question, A always uses expert 7, 19, and the two shared experts. That means 7 and 19 have a high co-activation count (via the shared ones too). We will identify such patterns and compare if C has a similar cluster (maybe expert 5 and 22 in C form the “math pair” analogous to 7 and 19 in A). We can measure cluster similarity by how many expert indices from A’s cluster correspond (via some mapping) to those in C’s cluster. Or simply qualitatively match them by task specialization known from Experiment 1\.

* **Router Weight Similarity:** Although more about parameters, here we consider directly comparing the **router gating weight vectors** of experts. Each expert $e$ has an associated gating weight vector (part of the larger router matrix): it’s the vector in input-hidden space that, when dotted with a token’s hidden state, yields the logit for that expert. If two experts often co-activate, their weight vectors might be related (or their input domains overlap). We can do a PCA or t-SNE on all 64 gating vectors for each model and visualize them in 2D. Possibly, in model A, the shared experts’ weight vectors could be positioned centrally or covering broad directions, whereas others might cluster by domain. For model C vs B, we can compare these visualizations: does C’s gating weight space look structurally more like A’s? Also, we can attempt to directly **match gating weights**: compute cosine similarity between every pair of an expert weight from A and an expert weight from C (and B). If C learned some gating functions similar to A’s, we’d find some high similarity pairs. If B is random, fewer matches. This ties into Experiment 1’s idea of matching specialization by weights rather than usage. We might incorporate that analysis here: e.g., find the permutation of C’s experts that maximizes weight vector alignment with A’s experts and measure the average cosine similarity. Compare to B.

* **Visualization:** For co-activations, a **heatmap of the co-activation matrix** for each model (maybe focusing on top 20 experts for clarity, or sorting experts by overall usage). We can highlight the shared experts in A and see the distinctive pattern. Then mark any analogous experts in C’s heatmap if present. We’ll plot a **bar chart of token participation rates** for each expert (essentially expert frequency from Experiment 1, but here to spot outliers near 100%). Another plot is a **network graph** where each expert is a node and an edge connects two experts if they co-activate frequently (edge weight \= co-activation count). Using a graph layout algorithm, we might see communities. A’s graph would have the shared experts connected to all nodes (hub-and-spoke), and possibly other communities. C’s graph should reveal if it formed hubs. B’s might be more fragmented or less clear structure. The gating weight vectors can be visualized via **t-SNE scatter**: we plot A, B, C’s gating vectors in the same 2D space (maybe color by model or by known task domain from Experiment 1). If C’s points overlap with A’s or fall into similar clusters, that’s telling. Alternatively, show separate scatter plots per model but annotated by likely task specialization (derived earlier), to see if similar groupings occur. We might also include a **dendrogram of experts** based on gating vector similarity for each model, to compare their hierarchies.

**Expected Outcomes:** Model A’s co-activation analysis will clearly identify the 2 shared experts as ubiquitous collaborators (co-activating with all others \~100% of the time). We expect to see that model C has one or two experts that are used in an outsized proportion of tokens, indicating it learned to have generalists akin to shared experts. If the distillation strongly encoded that behavior, possibly exactly 2 experts in C might appear extremely frequently. If distillation was weaker on that aspect, we might see maybe 1 that’s very frequent, or a few that are moderately frequent. Model B likely has none that approach the always-on usage (since it wasn’t trained with that concept). So a stark difference would be: in C, the top expert usage might be, say, 60-70% of tokens, whereas in B the top expert is maybe 20% of tokens. The co-activation heatmap for C might show a pseudo-hub pattern: one row/col with high values (if one expert pairs with many others frequently). This would answer if **C reused shared experts**.

In terms of other co-activation clusters, we might find that both A and C have similar grouping – e.g., if certain experts in A often co-activated for complex multi-step reasoning (just a hypothetical), maybe C also has a corresponding group. We could identify such by correlating the co-activation matrices: compute the correlation between $M^A$ and $M^C$ (after potential row/column permutation alignment). A higher correlation for A–C than A–B is expected if C learned similar pairwise usage relationships. Also, **router weight similarity**: if we find direct matches, say expert 10 in C has a gating weight vector 0.9 cosine similarity to expert 3 in A, that strongly implies it learned to detect the same input pattern as A’s expert 3 did. We expect more such high similarities for C than for B. This ties back to specialization: A’s expert 3 might be specialized in, say, Python code. If C’s expert 10 has a very similar gating weight, it likely responds to Python code in the same way. B’s gating weights might have shifted or not cover the space similarly. So likely outcome: cluster plots show C’s gating weights grouping in a way that mirrors A’s (maybe even intermix if plotted together), whereas B’s might be somewhat different – possibly less clearly clustered if fine-tuning didn’t enforce it strongly. Summarizing, we expect that **C’s expert collaboration graph and gating semantics align more with A’s**, reinforcing the conclusion that C was derived from A, while B, lacking that guidance, diverges in these fine-grained behaviors.

## **Experiment 5: Performance and Behavior under Perturbations (Control Experiments)**

To bolster our findings, we consider some control variations to ensure differences truly arise from distillation and not other factors (like model size, training randomness, etc.):

* **Ablation: Randomizing Router** – As a sanity check, if we randomize or shuffle expert assignments, all these metrics (entropy, overlap, CKA, etc.) collapse or change drastically, indicating our metrics are sensitive to meaningful routing. We won’t do this to the main models, but perhaps take model C and shuffle its expert indices to see that the overlap with A goes to essentially 0, proving that any high overlap we found was not a fluke.

* **Cross-Dataset Consistency:** We’ll run key comparisons on both the Tülu dataset and the calibration set. If C’s alignment with A is genuine, it should hold in the calibration set (even though those data weren’t seen during training, for both B and C). If we found something was only true on training data and not on calibration, that could mean it was a direct result of overfitting to the distillation process on training tasks. We expect consistent trends.

* **Distillation Temperature & Smoothness:** Knowledge distillation often uses a “temperature” to soften the teacher probabilities【29†L13-L22】. If a high temperature (smoother targets) was used, the student might be less confident in general. We might analyze if C’s output entropy is higher (less confident) than B’s on direct questions, as a side effect. If so, that’s not necessarily a bad sign, but worth noting as a difference due to distillation (soft targets make students less overconfident【29†L7-L15】).

* **Confounding: Model Depth and Capacity:** Model A has more layers (27 vs 16\) and uses more experts per token (8 vs 8, though with shared experts). Some differences in routing could be simply because a shallower model might behave differently. To isolate distillation effects, one could also compare B vs an identical architecture model trained from scratch on the same data to ensure B’s differences aren’t because it’s 16 layers vs teacher’s 27\. However, since B and C share architecture, comparing them is fair. We should mention that any observed alignment of C to A **above and beyond** B is attributed to distillation.

* **Statistical Significance:** For metrics like overlap, CKA, etc., we will conduct statistical tests (where applicable) to confirm differences (e.g., a paired t-test on token overlap scores for C vs B relative to A, yielding p-value). Given no compute constraints, we can get large sample sizes, making it easier to detect significant differences.

* **Edge Cases:** We might examine cases where B outperforms C or behaves differently (maybe B, being directly fine-tuned on instructions, could handle some prompts in a more straightforward way). If any such are found, we’ll discuss them. But as our focus is on exposing *subtle distillation effects*, we anticipate largely that C will show more “A-like” behavior than B on these metrics.

**Expected Outcomes:** The control checks should validate that patterns we attribute to distillation (like higher A–C similarity) are real and not artifacts. For example, if we align experts randomly, the expert overlap or gating weight similarity should drop, confirming our overlap measure isn’t trivially high by chance. Cross-dataset tests likely show the same qualitative differences between B and C relative to A. If there’s any drop-off, we’d note that maybe C was very aligned on training distribution but slightly less so on calibration – which could hint at mild overfitting in distillation (something to be careful about). Overall, after these experiments, we expect to have a comprehensive picture where **model C consistently aligns with model A on multiple axes (routing, usage, internal representations, expert interactions) much more than model B does**, thus revealing it was distilled from A.

## **Synthesis: Answering the Big Question**

After all the above experiments, we will compile the evidence to conclusively determine whether model C has been distilled from model A or not, and how it differs from the baseline fine-tuned model B. Each experiment corresponds to a research question and yields specific insights:

* **Expert Specialization (Exp 1):** If C mirrors A in which experts handle which tasks (and overall usage skew), it suggests C inherited A’s division of labor among experts. B likely won’t show that match.

* **Routing Patterns (Exp 2):** If C selects experts per token similarly to A (high overlap, similar gating confidence profiles), that’s a strong sign of distillation of the routing policy. If B differs substantially here, it confirms those similarities are not trivial.

* **Representation Similarity (Exp 3):** High CKA and matched expert representations between A and C (vs low for B) would mean C learned internal functions akin to A.

* **Expert Collaboration (Exp 4):** Similar co-activation networks and possibly presence of “always-on” experts in C (like A’s shared experts) is a subtle effect we’d only expect if C was imitating A’s strategy. B, lacking that concept, wouldn’t have it.

* **Controls (Exp 5):** These ensure the above differences indeed stem from the distillation process and aren’t explained by other confounds.

By the end, we will have multiple quantitative comparisons (entropy values, overlap percentages, CKA scores, etc.) and informative visualizations (heatmaps, graphs, similarity plots) that collectively **differentiate C from B in A-like behavior**. We anticipate the distilled model C will show **smoother routing, more aligned expert usage, and closer hidden state correspondence to A** – e.g., perhaps C uses a particular expert for coding tasks exactly as A would (which B might not), or C’s gating network has effectively learned the same input-\>expert mapping as A (reflected in weight similarities and token decisions). These subtle cues – like *lower routing entropy indicating confident expert assignment like the teacher*【3†L55-L63】, or *consistent reuse of a general expert indicating shared expert behavior* – are the smoking guns of distillation.

## **Expected Insights and Limitations**

Through this experimental framework, we expect to uncover how knowledge distillation in an MoE context goes beyond matching outputs to actually aligning the **modular sub-structures** (experts) of the student with the teacher. This can reveal, for instance, that the distilled student not only performs well but does so by internally “mimicking” the teacher’s specialization of experts and routing heuristics. We might observe that model C has **smoother gating decisions** (less brittle changes, potentially due to inheriting the teacher’s more stable router【2†L626-L634】) and uses its experts in a more **organized and teacher-like fashion** (e.g., balanced yet specialized usage, thanks to the teacher’s influence【26†L1-L4】). Additionally, if we see strong expert-to-expert alignment, it implies the knowledge in each expert of the teacher was at least partially transferred to a corresponding expert in the student – a fascinating fine-grained validation of distillation.

However, there are limitations and considerations:

* **Expert Index Alignment:** Experts are permutation-invariant to some extent; one could shuffle expert IDs in an MoE without affecting overall model function. Our analyses assume some natural alignment or attempt to find one. If model C learned the same functions but assigned them different indices, our direct comparisons might need careful alignment. We addressed this by matching via task usage or output similarities, but it’s possible we won’t get a perfect one-to-one match, and interpretation will need care (e.g., expert 5 in A corresponds to expert 12 in C, etc.).

* **Distillation Objective:** We assumed model C was distilled on the same data with only the teacher’s guidance. If any additional losses (like hidden state matching or auxiliary losses) were used, they could specifically affect some metrics (for instance, if C was trained with a loss to match A’s hidden layers, it would trivially boost CKA – we should then not be surprised by high CKA, but rather use that as a check that indeed it was enforced). We might not know all details of how C was trained. We assume standard distillation (matching logits). If it was purely logits, then any internal alignment we find is an emergent property, making it more convincing.

* **Baseline Differences:** Model B’s training (SFT on Tülu) might not push the MoE to its limits – maybe it didn’t train long enough for experts to specialize strongly, or maybe the tasks don’t force heavy expert differentiation. If B ended up with somewhat dormant experts or a degenerate gating (e.g., always choosing the first few experts because it wasn’t carefully balanced), then B could be an unfairly weak baseline. We assume SFT was reasonably done with maybe some auxiliary load balancing to use all experts, but it’s a point to consider. If B has issues (like mode-collapse on routing), some differences we see might be partially due to B’s gating not being well-trained rather than C being exceptionally like A. We would mention that possibility. Conversely, if B was heavily tuned, differences might be smaller, but we still expect systematic distinctions.

* **Visualization Clarity:** With 64 experts, some visualizations (full matrices) are large. We will need to present them smartly (maybe focusing on top experts or clustering). Also, multiple metrics could be redundant; we should highlight the clearest, most meaningful ones in the final report to avoid confusion.

* **Confounding from Architecture**: A has 27 layers vs C/B 16; also top-6 vs top-8 routing differences. We assume these mainly impact capacity, not the fundamental patterns; but for instance, top-8 vs top-6 means C/B pick more experts per token than A did. That could inherently cause differences (maybe less specialization because each token already spreads to 8 of 64 vs teacher’s 6 of 64). However, the presence of 2 shared in A’s 8 might make A effectively pick 6 unique \+ 2 general; while B/C pick 8 potentially unique. So actually B/C might spread more if anything. If we find C has lower entropy than B, that’s significant given it picks more experts – it might indicate it really keys in on a subset similarly to teacher. We will have to keep in mind these differences when interpreting results.

By addressing the research questions with these experiments, our structured plan will isolate whether model C’s MoE routing and expert behaviors bear the signature of having been **distilled from model A**. Each section of analysis (specialization, routing stats, hidden similarity, expert interactions) provides evidence that, when combined, paints a comprehensive comparison between the *distilled MoE student* (C) and the *fine-tuned baseline* (B). We expect to conclude that model C does show clear signs of its teacher’s influence – such as **expert alignment, smoother gating, and shared expert reuse** – which would not be present in model B. These findings not only answer the original question (distilled vs not) but also offer insights into *how* knowledge distillation manifests in large MoE models at a fine-grained level, a topic of both practical and theoretical interest in the era of expert-based LLMs.

## Lessons

1. **Never average anything across layer dimension (just because of same expert IDs)**. Expert IDs are irrelavent across MoE layers. For example, the expert 0 in layer 1 and the expert-1 in layer-2 are irrelevant.
